# 🧠 Akıllı Log Asistanı (AI-Powered Log Analyzer)

Bu proje, sistem yöneticilerinin manuel olarak incelemek zorunda kaldığı büyük boyutlu log dosyalarını AI ile analiz ederek, potansiyel anomalileri otomatik olarak tespit eden bir kurum içi log analiz aracıdır.

> 🎯 MVP Sürüm: Anomali tespiti, log yükleme ve görsel uyarı sistemi içerir.  
> 🏢 Kurum içi çalışacak şekilde tasarlanmıştır. Tüm bileşenler Docker üzerinde local olarak çalışır.

---

## 🚀 Özellikler

- ✅ CSV / LOG dosyası yükleme
- ✅ LLM destekli akıllı anomali tespiti (Llama 3.2)
- ✅ Kritik hata uyarısı ve detaylı açıklamalar
- ✅ Web arayüzü ile logları inceleme ve filtreleme
- ✅ Kurum içi dağıtım (offline kullanım)
- 🆕 Doğal dil işleme ile gelişmiş log analizi

---

## 📁 Proje Yapısı

log-analyzer/
├── app/
│ ├── main.py # FastAPI backend
│ ├── model.py # ML model yükleme ve tahmin
│ ├── utils.py # Veri işleme
├── frontend/
│ ├── public/ # Statik dosyalar
│ ├── src/ # React app (shadcn/ui ile)
├── data/
│ └── example.csv # Örnek log dosyası
├── Dockerfile
├── docker-compose.yml
├── README.md

yaml
Kopyala
Düzenle

---

## 🛠️ Kurulum ve Çalıştırma

### ⚡ Hızlı Başlangıç (3 dakikada çalışır!)

#### **1. Ollama Kurulumu (AI Motor)**

**Windows:**
```powershell
# PowerShell'i yönetici olarak açın
winget install ollama
# veya
# https://ollama.ai/download adresinden indirin
```

**macOS/Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### **2. AI Modelini İndirin**
```bash
# Llama 3.2 modelini indirin (2-3 dakika sürer)
ollama pull llama3.2

# Ollama servisini başlatın
ollama serve
```

#### **3. Python Environment Hazırlığı**
```bash
# Python 3.11 kullanmanız önerilir (uyumluluk için)
conda create -n loggy-py311 python=3.11 -y
conda activate loggy-py311
```

#### **4. Projeyi Çalıştırın**

**Otomatik Başlatma (Windows):**
```bash
# Proje dizinine gidin
cd C:\path\to\Loggy

# Tek komutla başlatın
start.bat
```

**Manuel Başlatma:**
```bash
# Backend'i başlatın
cd backend
pip install -r requirements.txt
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Yeni terminal'de frontend'i başlatın
cd frontend
npm install --legacy-peer-deps
npm start
```
### 🔍 **Sistem Durumu Kontrolü**

```bash
# 1. Ollama çalışıyor mu?
ollama --version
ollama list  # Modeller listelensin

# 2. Backend çalışıyor mu?
curl http://localhost:8000/health
# Yanıt: {"status":"healthy","database":"connected","llm":"ready","model":"llama3.2"}

# 3. Frontend çalışıyor mu?
# http://localhost:3000 adresini tarayıcıda açın
```

### 🌐 **Erişim Adresleri**

- **Frontend:** http://localhost:3000
- **Backend API:** http://localhost:8000
- **API Dokümantasyonu:** http://localhost:8000/docs
- **Health Check:** http://localhost:8000/health

### 🔧 **Sorun Giderme**

**Problem: "Ollama bulunamadı"**
```bash
# PATH'e ekleyin
$env:PATH += ";C:\Users\$env:USERNAME\AppData\Local\Programs\Ollama"
```

**Problem: "Model bulunamadı"**
```bash
ollama pull llama3.2  # Modeli yeniden indirin
ollama serve          # Servisi başlatın
```

**Problem: "Port 8000 kullanımda"**
```bash
# Çalışan Python processlerini durdurun
tasklist | findstr python
taskkill /PID [PID_NUMBER] /F
```

**Problem: "Frontend npm hatası"**
```bash
cd frontend
rm -rf node_modules package-lock.json
npm install --legacy-peer-deps
```

### 📊 **Nasıl Kullanılır?**

1. **Web arayüzüne gidin:** http://localhost:3000
2. **Log dosyanızı yükleyin:** `.csv`, `.log` veya `.txt` formatında
3. **Analizi başlatın:** "Analiz Et" butonuna tıklayın
4. **Sonuçları inceleyin:** LLM otomatik olarak anomalileri tespit edecek

### 🧪 **Hızlı Test**

```bash
# Test scriptini çalıştırın
python test_llm.py

# Örnek çıktı:
# ✅ Analiz başarılı!
# 📊 Toplam satır: 5
# ⚠️  Anomali sayısı: 2
# 🚨 Kritik sayısı: 2
# 📈 Anomali oranı: 40.00%
```

### 📁 **Örnek Log Dosyaları**

- `data/sample_logs.csv` - Test için hazır log dosyası
- Desteklenen formatlar: CSV, LOG, TXT
- Maksimum dosya boyutu: 50MB

🧪 Test Log Verisi
LogPai HDFS Dataset

LogPai BGL Dataset (Berkeley)

data/ klasörüne .csv olarak koyabilirsiniz.

🔒 Güvenlik ve Gizlilik
Bu uygulama sadece kurum içi çalışacak şekilde tasarlanmıştır

Kullanıcı verisi dışarı aktarılmaz

IP, kullanıcı adı gibi PII alanları maskelemeye uygundur

Kullanıcı oturumu gerektirmez (tek kullanıcı için MVP)

🧩 Teknolojiler
Backend: Python, FastAPI

AI/LLM: Ollama + Llama 3.2 (yerel, ücretsiz)

Frontend: React.js, TailwindCSS, Shadcn/UI

Dağıtım: Docker, Docker Compose

Veritabanı: SQLite (yerel, hafif)

📬 İletişim
Bu proje, sistem güvenliği ve log analizi süreçlerini modernleştirmek amacıyla geliştirilmiştir.
Her türlü geri bildirim veya geliştirme öneriniz için bizimle iletişime geçebilirsiniz.