# ğŸš€ Ollama Kurulum Rehberi

## 1. Ollama'yÄ± Ä°ndirin ve Kurun

### Windows:
```bash
# PowerShell'de Ã§alÄ±ÅŸtÄ±rÄ±n
curl.exe -fsSL https://ollama.ai/install.ps1 | powershell
```

### MacOS:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### Linux:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

## 2. Llama 3.2 Modelini Ä°ndirin

Ollama kurulduktan sonra terminal/PowerShell'de:

```bash
ollama pull llama3.2
```

Bu iÅŸlem 2-3 dakika sÃ¼rebilir (model boyutu ~2GB).

## 3. Ollama Servisini BaÅŸlatÄ±n

```bash
ollama serve
```

Bu komut Ollama'yÄ± `http://localhost:11434` adresinde baÅŸlatÄ±r.

## 4. Test Edin

Yeni bir terminal/PowerShell penceresi aÃ§Ä±n:

```bash
ollama run llama3.2
```

Ã‡alÄ±ÅŸÄ±yorsa ">>> " promptu gÃ¶rmelisiniz. `/bye` yazarak Ã§Ä±kabilirsiniz.

## 5. Projeyi BaÅŸlatÄ±n

```bash
cd backend
pip install -r requirements.txt
python -m uvicorn app.main:app --reload
```

## âš¡ HÄ±zlÄ± Test

Backend Ã§alÄ±ÅŸtÄ±ktan sonra:
- http://localhost:8000/health - API durumunu kontrol edin
- http://localhost:8000/api/upload - Log dosyasÄ± yÃ¼kleyin
- Analiz sonuÃ§larÄ± otomatik olarak LLM ile gelecek

## ğŸ”§ Sorun Giderme

**Ollama baÄŸlanamÄ±yorsa:**
1. `ollama serve` komutunu Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zdan emin olun
2. Firewall'Ä±n port 11434'Ã¼ engellemediÄŸini kontrol edin
3. Model indirildi mi: `ollama list`

**Model yavaÅŸsa:**
- GPU kullanÄ±mÄ± iÃ§in NVIDIA kartÄ±nÄ±z varsa otomatik algÄ±lar
- CPU'da da Ã§alÄ±ÅŸÄ±r ama biraz yavaÅŸ olabilir

## ğŸ“Š Performans

- **Ä°lk Ã§alÄ±ÅŸtÄ±rma:** 10-15 saniye (model yÃ¼klenir)
- **Sonraki analizler:** 2-5 saniye
- **Dosya boyutu:** 50MB'a kadar desteklenir
